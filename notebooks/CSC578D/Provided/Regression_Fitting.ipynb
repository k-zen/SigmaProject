{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Underfitting vs. Overfitting\n",
    "============================\n",
    "\n",
    "Adapted from: http://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html\n",
    "\n",
    "This example demonstrates the problems of underfitting and overfitting and\n",
    "how we can use linear regression with polynomial features to approximate\n",
    "nonlinear functions. The plot shows the function that we want to approximate,\n",
    "which is a part of the cosine function. In addition, the samples from the\n",
    "real function and the approximations of different models are displayed. The\n",
    "models have polynomial features of different degrees. We can see that a\n",
    "linear function (polynomial with degree 1) is not sufficient to fit the\n",
    "training samples. This is called **underfitting**. A polynomial of degree 4\n",
    "approximates the true function almost perfectly. However, for higher degrees\n",
    "the model will **overfit** the training data, i.e. it learns the noise of the\n",
    "training data.\n",
    "We evaluate quantitatively **overfitting** / **underfitting** by using\n",
    "cross-validation. We calculate the mean squared error (MSE) on the validation\n",
    "set, the higher, the less likely the model generalizes correctly from the\n",
    "training data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_fun(X):\n",
    "    return np.cos(1.5 * np.pi * X)\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "n = 30\n",
    "\n",
    "# generating a train dataset\n",
    "X = np.random.rand(n,1)\n",
    "y = true_fun(X) + np.random.randn(n,1) * 0.1  # second term is noise\n",
    "\n",
    "plt.scatter(X,y)\n",
    "plt.show()\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "polynomial_features.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try 1, 4, 15\n",
    "degree = 15\n",
    "\n",
    "polynomial_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "linear_regression = LinearRegression()\n",
    "ridge_regression = Ridge(alpha=.05) # regression with regularization. \n",
    "                                    # alpha is the \"lambda\" used in the slides\n",
    "    \n",
    "X_poly = polynomial_features.fit_transform(X)\n",
    "linear_regression.fit(X_poly, y)\n",
    "ridge_regression.fit(X_poly, y)\n",
    "\n",
    "X_test = np.linspace(0, 1, 100).reshape(100,1) # 100 linearly spaced numbers from 0 to 1.\n",
    "X_test_poly = polynomial_features.fit_transform(X_test)\n",
    "y_test = true_fun(X_test)\n",
    "\n",
    "y_test_regression_pred = linear_regression.predict(X_test_poly)\n",
    "y_test_ridge_pred = ridge_regression.predict(X_test_poly)\n",
    "\n",
    "# (1)\n",
    "plt.plot(X_test, y_test_regression_pred, \"b\") # Blue: model prediction on test set\n",
    "# (2)\n",
    "# plt.plot(X_test, y_test_ridge_pred, \"b\")\n",
    "\n",
    "plt.plot(X_test, y_test, \"r\")      # Red: test set generated using true function\n",
    "plt.scatter(X, y)             # Blue dots: training instances\n",
    "\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(-1.5,1.5)\n",
    "plt.title(\"Degree {}\".format(degree))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we observe\n",
    "--\n",
    "\n",
    "When we set the polynomial features highest degree to 15, linear regression produces an overfitted line, which fits the training data (blue dots) well but does a poor job on the test set (red line).\n",
    "\n",
    "Now, comment out (1) and uncomment (2) in the python code. Ridge regression will be used now. This is regression using regularization as explained in class. \n",
    "What we observe now is that overfitting is almost eliminated. \n",
    "\n",
    "Experiments\n",
    "--\n",
    "\n",
    "You can experiment in the above code with different polynomial features degrees, and other alpha (lambda) values, such as the default, one, or 0.1, 0.01, 0.0001, etc. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
