{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Linear Regression Python Notebook\n",
    "## Chapter 5: Machine Learning\n",
    "### Python Data Science / Page 390\n",
    "### Note:\n",
    "- Linear regression using Python's **scikit-learn** library.\n",
    "- See notebook at **CSC 578D**, for a detailed explanation of **Linear Regression** and a custom implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from IPython.display import Math\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", module=\"scipy\", message=\"^internal gelsd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# generate the data\n",
    "rng = np.random.RandomState(1) # generate a 1-D array of random numbers\n",
    "\n",
    "# load data into X and Y\n",
    "x = 10 * rng.rand(50) # generate 50 uniform random numbers [0,1)\n",
    "y = 2 * x - 5 + rng.randn(50) # compute function y = f(x) = 2x - 5 + C, where C is a normal random number N(0,1)\n",
    "\n",
    "# plot the data\n",
    "plt.scatter(x, y)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Model Formula:\n",
    "$\\hat{y} = b + w_1 x_1 + ... + w_n x_n$, where:\n",
    "\n",
    "1. $b$ = Bias / same as y-intercept or $b_0$\n",
    "1. $w_1$ = weight of feature 1 / same as m (slope) or b1\n",
    "1. $x_1$ = feature 1 or known input\n",
    "1. $\\hat{y}$ = predicted label or desired output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = LinearRegression(fit_intercept=True) # fit_intercept=True calculate Y-intercept\n",
    "# generate the linear regression model\n",
    "model.fit(x[:, np.newaxis], y)\n",
    "print(\"Feature 1 slope (w1):\", model.coef_[0])\n",
    "print(\"Model intercept (w0):\", model.intercept_)\n",
    "\n",
    "# debug\n",
    "print(\"\")\n",
    "print(\"Model fitting parameters:\")\n",
    "print(\"X Data Matrix {}\".format(np.shape(x[:, np.newaxis])))\n",
    "print(\"Y Data Matrix {}\".format(np.shape(y)))\n",
    "print(\"\")\n",
    "\n",
    "xfit = np.linspace(0, 10, 1000) # 1000 evenly spaced numbers [0,10]\n",
    "yfit = model.predict(xfit[:, np.newaxis]) # make the predictions based on the model generated previously\n",
    "\n",
    "# generate both plots: scatter and line\n",
    "plt.scatter(x, y)\n",
    "plt.plot(xfit, yfit)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# make an individual prediction based on the model\n",
    "prediction = model.predict(np.array([[12]]))[0]\n",
    "print(\"Individual Prediction:\")\n",
    "print(\"For x=12, ŷ={}\".format(prediction))\n",
    "\n",
    "xfit = np.linspace(0, 12, 1000)\n",
    "yfit = model.predict(xfit[:, np.newaxis])\n",
    "\n",
    "# generate scatter plot\n",
    "plt.scatter(np.append(x, [12]), np.append(y, prediction))\n",
    "plt.plot(xfit, yfit)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes:\n",
    "\n",
    "## Least Squares Criterion:\n",
    "A line that fits the data \"best\" will be one for which the n prediction errors — one for each observed data point — are as small as possible in some overall sense. The “Least Squares Criterion” which says to “minimize the sum of the squared prediction errors.” That is:\n",
    "\n",
    "1. The equation of the best fitting line is: $\\hat{y_i} = b_0 + b_1x_i$\n",
    "2. We just need to find the values $b_0$ and $b_1$ that make the sum of the squared prediction errors the smallest it can be.\n",
    "3. That is, we need to find the values $b_0$ and $b_1$ that minimize:\n",
    "\n",
    "$$Q = \\sum_{i=1}^{n} (y_i - \\hat{y})^2$$\n",
    "￼\n",
    "The goal is to reduce Q as much as we can.\n",
    "\n",
    "## Nomenclature:\n",
    "1. $b_0$ or $b$: y-intercept (value of y when all the predictors equal zero)\n",
    "1. $w_n$: weight or slope of feature n\n",
    "1. $e_i$: i-th prediction error, equal to $y_i - \\hat{y_{i}}$\n",
    "1. MSE (“mean square error”): mean square prediction error (or residual error)\n",
    "1. $x$: a predictor, explanatory, or independent variable in a linear regression model\n",
    "1. $\\bar{x}$: sample mean of x\n",
    "1. $y$: the response, outcome, or dependent variable in a linear regression model\n",
    "1. $\\bar{y}$: sample mean of y (ignoring any predictors)\n",
    "1. $\\hat{y}$: predicted or fitted value of y in a linear regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
