{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:right\">\n",
    "    <img alt=\"K-Zen\" title=\"K-Zen\" src=\"http://apkc.net/img/images/K-Zen_6.jpeg\" style=\"width:50px;height:auto;border-radius:50%;\"/>\n",
    "    <br/>\n",
    "    <b>Author:</b> Andreas P. Koenzen (akc at apkc.net) / <a href=\"http://www.apkc.net\">www.apkc.net</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSC 578D / Data Mining / Fall 2018 / University of Victoria\n",
    "\n",
    "## Assignment 01 / Problem 03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run '../../../../jupyter_imports.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miscellaneous configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run '../../../../miscellaneous_config.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment variables and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run '../../../../env_variables.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "url_data = rq.get('http://www.apkc.net/data/weka/contact-lenses.arff').text\n",
    "data = arff.loadarff(StringIO(url_data))\n",
    "df = pd.DataFrame(data[0], index=pd.Index(np.arange(24) + 1), dtype='object')\n",
    "\n",
    "# Convert all data in the columns to strings instead of binary objects.\n",
    "string_df = df.select_dtypes([np.object]).stack().str.decode('UTF-8').unstack()\n",
    "for col in string_df:\n",
    "    df[col] = string_df[col]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "(4 points) Classify using Naïve Bayes method (on contact lenses data) the data item: pre-presbyopic, hypermetrope, yes, reduced, ? Then, check your solution with Weka (the data file is included with Weka).\n",
    "\n",
    "### The model computed by Weka for this problem is the following\n",
    "\n",
    "```\n",
    "                      Class\n",
    "Attribute              soft   hard   none\n",
    "                     (0.22) (0.19) (0.59)\n",
    "==========================================\n",
    "age\n",
    "  young                 3.0    3.0    5.0\n",
    "  pre-presbyopic        3.0    2.0    6.0\n",
    "  presbyopic            2.0    2.0    7.0\n",
    "  [total]               8.0    7.0   18.0\n",
    "\n",
    "spectacle-prescrip\n",
    "  myope                 3.0    4.0    8.0\n",
    "  hypermetrope          4.0    2.0    9.0\n",
    "  [total]               7.0    6.0   17.0\n",
    "\n",
    "astigmatism\n",
    "  no                    6.0    1.0    8.0\n",
    "  yes                   1.0    5.0    9.0\n",
    "  [total]               7.0    6.0   17.0\n",
    "\n",
    "tear-prod-rate\n",
    "  reduced               1.0    1.0   13.0\n",
    "  normal                6.0    5.0    4.0\n",
    "  [total]               7.0    6.0   17.0\n",
    "```\n",
    "\n",
    "### Notes\n",
    "\n",
    "- 3 significant digits are used for all results.\n",
    "- results are rounded up if 4th significant digit is >= 5.\n",
    "- Laplace normalization should be applied to avoid *zero-frequency* problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes' rule\n",
    "\n",
    "$$P(C \\mid f) = \\frac{P(f \\mid C)P(C)}{P(f)}$$\n",
    "<br>\n",
    "<br>\n",
    "C = Class to predict; f = Feature to use\n",
    "<br>\n",
    "<br>\n",
    "The formula above works for only one feature/attribute. We need a formula to allow multiple attributes. So in Naïve Bayes we applied the intersection of many features **given** a certain class, over the evidence (or normalization factor), which is negligible, so we replace it with $\\alpha = \\frac{1}{E}$ and store it for later, when we need to compute the probability of each class **given** the features.\n",
    "\n",
    "### Naïve Bayes\n",
    "\n",
    "**Notation:**\n",
    "The coma (,) in Bayes' rule can be used as the AND operator, or the intersection of two or more events. i.e. $P(A,B) = P(B,A) = P(A \\mid B)P(B) = P(B \\mid A)P(A)$\n",
    "<br>\n",
    "<br>\n",
    "$$P(C_{k} \\mid f_{1},...,f_{n}) \\propto P(f_{1},...,f_{n},C_{k})$$\n",
    "<br>\n",
    "<br>\n",
    "Now the numerator $P(f_{1},...,f_{n},C_{k})$ (Likelihood) can be expanded using the chain rule into:\n",
    "<br>\n",
    "<br>\n",
    "$$P(f_{1},...,f_{n},C_{k}) = P(f_{i} \\mid f_{i+1},...,f_n,C_{k})$$\n",
    "<br>\n",
    "<br>\n",
    "$$P(f_{1},...,f_{n},C_{k}) = P(f_{1} \\mid f_{n+1},...,f_{n},C_{k}) ... P(f_{n-1} \\mid f_{n},C_{k}) P(f_{n} \\mid C_{k}) P(C_{k})$$\n",
    "<br>\n",
    "<br>\n",
    "Now if we consider the independence of events, as in: $P(A,B) = P(A)P(B)$, we have that:\n",
    "<br>\n",
    "<br>\n",
    "$$P(f_{i} \\mid f_{i+1},...,f_{n},C_{k}) = P(f_{i} \\mid C_{k})$$\n",
    "<br>\n",
    "<br>\n",
    "Hence:\n",
    "<br>\n",
    "<br>\n",
    "$$P(C_{k} \\mid f_{1},...,f_{n}) = \\frac{1}{E} \\times P(C_{k}) \\prod_{i=1}^{n} P(f_{i} \\mid C_{k}) = P(C_{k}) \\prod_{i=1}^{n} P(f_{i} \\mid C_{k}) \\times \\alpha$$\n",
    "<br>\n",
    "<br>\n",
    "Where $E = P(f)$ is the normalising factor computed using the Law of Total Probability:\n",
    "<br>\n",
    "<br>\n",
    "$$E = P(f) = \\sum_{k}^{} P(\\textbf{f} \\mid C_{k}) P(C_{k})$$\n",
    "<br>\n",
    "<br>\n",
    "Now, we can obviate E until the last moment where we can find its value to calculate the final probabilities. If we sum the prediction of all classes **given** the features, then they should add to 1.\n",
    "<br>\n",
    "<br>\n",
    "$$\\sum_{k}^{} P(C_{k} \\mid f) = 1$$\n",
    "\n",
    "### Naïve Bayes classifier\n",
    "\n",
    "The discussion so far has derived the independent feature model, that is, the naive Bayes probability model. The naive Bayes classifier combines this model with a decision rule. One common rule is to pick the hypothesis that is most probable; this is known as the *maximum a posteriori* or MAP decision rule. The corresponding classifier, a Bayes classifier, is the function that assigns a class label $\\hat{y} = C_k$ for some k as follows:\n",
    "<br>\n",
    "<br>\n",
    "$$\\hat{y} = {\\underset{k \\in \\{1, \\dots ,K\\}}{\\operatorname{argmax}} P(C_k) \\prod_{i=1}^{n} P(x_i \\mid C_k). \\quad\\quad (1)}$$\n",
    "\n",
    "### Calculation\n",
    "\n",
    "$P(\\text{contact-lenses=none} \\mid \\text{E}) = P(\\text{age=pre-presbyopic} \\mid \\text{contact-lenses=none}) \\times P(\\text{spectacle-prescrip=hypermetrope} \\mid \\text{contact-lenses=none}) \\times P(\\text{astigmatism=yes} \\mid \\text{contact-lenses=none}) \\times P(\\text{tear-prod-rate=reduced} \\mid \\text{contact-lenses=none}) \\times P(\\text{contact-lenses=none}) \\times \\alpha$\n",
    "<br>\n",
    "<br>\n",
    "$P(\\text{contact-lenses=none} \\mid \\text{E}) = \\frac{5+1}{15+3} \\times \\frac{8+1}{15+2} \\times \\frac{8+1}{15+2} \\times \\frac{12+1}{15+2} \\times \\frac{15+1}{24+3} \\times \\alpha = 0.04\\alpha$\n",
    "<br>\n",
    "<br>\n",
    "$P(\\text{contact-lenses=soft} \\mid \\text{E}) = P(\\text{age=pre-presbyopic} \\mid \\text{contact-lenses=soft}) \\times P(\\text{spectacle-prescrip=hypermetrope} \\mid \\text{contact-lenses=soft}) \\times P(\\text{astigmatism=yes} \\mid \\text{contact-lenses=soft}) \\times P(\\text{tear-prod-rate=reduced} \\mid \\text{contact-lenses=soft}) \\times P(\\text{contact-lenses=soft}) \\times \\alpha$\n",
    "<br>\n",
    "<br>\n",
    "$P(\\text{contact-lenses=soft} \\mid \\text{E}) = \\frac{2+1}{5+3} \\times \\frac{3+1}{5+2} \\times \\frac{1}{5+2} \\times \\frac{1}{5+2} \\times \\frac{5+1}{24+3} \\times \\alpha = 0.001\\alpha$\n",
    "<br>\n",
    "<br>\n",
    "$P(\\text{contact-lenses=hard} \\mid \\text{E}) = P(\\text{age=pre-presbyopic} \\mid \\text{contact-lenses=hard}) \\times P(\\text{spectacle-prescrip=hypermetrope} \\mid \\text{contact-lenses=hard}) \\times P(\\text{astigmatism=yes} \\mid \\text{contact-lenses=hard}) \\times P(\\text{tear-prod-rate=reduced} \\mid \\text{contact-lenses=hard}) \\times P(\\text{contact-lenses=hard}) \\times \\alpha$\n",
    "<br>\n",
    "<br>\n",
    "$P(\\text{contact-lenses=hard} \\mid \\text{E}) = \\frac{1+1}{4+3} \\times \\frac{1+1}{4+2} \\times \\frac{4+1}{4+2} \\times \\frac{1}{4+2} \\times \\frac{4+1}{24+3} \\times \\alpha = 0.002\\alpha$\n",
    "<br>\n",
    "<br>\n",
    "Now if $\\alpha = \\frac{1}{P(E)}$ then:\n",
    "<br>\n",
    "<br>\n",
    "$\\frac{(0.001 + 0.002 + 0.04)}{P(E)} = 1.0 \\implies P(E) = (0.001 + 0.002 + 0.04) = 0.043$\n",
    "<br>\n",
    "<br>\n",
    "Now we calculate each individual probability and pick the greatest probability according to (1):\n",
    "<br>\n",
    "<br>\n",
    "$P(\\text{contact-lenses=none} \\mid \\text{E}) = \\frac{0.04}{0.043} = 93\\%$\n",
    "<br>\n",
    "<br>\n",
    "$P(\\text{contact-lenses=soft} \\mid \\text{E}) = \\frac{0.001}{0.043} = 2.3\\%$\n",
    "<br>\n",
    "<br>\n",
    "$P(\\text{contact-lenses=hard} \\mid \\text{E}) = \\frac{0.002}{0.043} = 4.7\\%$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final solution\n",
    "\n",
    "Weka classifies the entry *pre-presbyopic,hypermetrope,yes,reduced* as being of class **none**, with a probability of 92.5% (0.925).\n",
    "<br>\n",
    "<br>\n",
    "According to this computation, the instance *pre-presbyopic,hypermetrope,yes,reduced* would be classified as belonging to class **none** with a probability of ~93%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
